{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5b4bef",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d36c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f85f7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Understanding M-CHAT Questionnaire\n",
    "\n",
    "### What is M-CHAT?\n",
    "The **Modified Checklist for Autism in Toddlers (M-CHAT)** is a screening tool to assess risk for Autism Spectrum Disorder (ASD). It consists of 10 yes/no questions (A1-A10) answered by parents/caregivers.\n",
    "\n",
    "### M-CHAT Questions:\n",
    "1. **A1**: Does your child enjoy being swung, bounced on your knee, etc.?\n",
    "2. **A2**: Does your child take an interest in other children?\n",
    "3. **A3**: Does your child like climbing on things?\n",
    "4. **A4**: Does your child enjoy playing peek-a-boo/hide-and-seek?\n",
    "5. **A5**: Does your child ever pretend (e.g., care for dolls, talk on phone)?\n",
    "6. **A6**: Does your child use his/her index finger to point, to ask for something?\n",
    "7. **A7**: Does your child use his/her index finger to point to indicate interest?\n",
    "8. **A8**: Can your child play properly with small toys without just mouthing/fiddling?\n",
    "9. **A9**: Does your child ever bring objects over to show you?\n",
    "10. **A10**: Does your child look you in the eye for more than a second or two?\n",
    "\n",
    "**Scoring**: 0 or 1 for each question. Higher total scores may indicate higher ASD risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90189f7c",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load autism dataset\n",
    "autism_data = pd.read_csv('data/autism.csv')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"AUTISM DATASET INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset Shape: {autism_data.shape}\")\n",
    "print(f\"Total Samples: {autism_data.shape[0]}\")\n",
    "print(f\"Total Features: {autism_data.shape[1]}\")\n",
    "print(f\"\\nColumn Names:\")\n",
    "print(autism_data.columns.tolist())\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "autism_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb711615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_values = autism_data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"✓ No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1941501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column\n",
    "target_col = None\n",
    "for candidate in ['Class/ASD', 'ASD', 'Class', 'class']:\n",
    "    if candidate in autism_data.columns:\n",
    "        target_col = candidate\n",
    "        break\n",
    "\n",
    "print(f\"\\nTarget Column: {target_col}\")\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(autism_data[target_col].value_counts())\n",
    "print(f\"\\nClass Distribution (Percentage):\")\n",
    "print(autism_data[target_col].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83459b79",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. M-CHAT Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7babf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate M-CHAT total score for each individual\n",
    "mchat_cols = [f'A{i}_Score' for i in range(1, 11)]\n",
    "autism_data['MCHAT_Total_Score'] = autism_data[mchat_cols].sum(axis=1)\n",
    "\n",
    "print(\"\\nM-CHAT Score Statistics:\")\n",
    "print(autism_data['MCHAT_Total_Score'].describe())\n",
    "\n",
    "# Analyze score distribution by class\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"M-CHAT SCORES BY ASD CLASS\")\n",
    "print(\"=\"*80)\n",
    "for class_label in autism_data[target_col].unique():\n",
    "    subset = autism_data[autism_data[target_col] == class_label]['MCHAT_Total_Score']\n",
    "    print(f\"\\nClass {class_label} ({'ASD' if class_label == 1 else 'No ASD'}):\")\n",
    "    print(f\"  Mean Score: {subset.mean():.2f}\")\n",
    "    print(f\"  Median Score: {subset.median():.2f}\")\n",
    "    print(f\"  Std Dev: {subset.std():.2f}\")\n",
    "    print(f\"  Min-Max: {subset.min()}-{subset.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize M-CHAT score distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Histogram by class\n",
    "for class_label in autism_data[target_col].unique():\n",
    "    subset = autism_data[autism_data[target_col] == class_label]['MCHAT_Total_Score']\n",
    "    axes[0].hist(subset, alpha=0.6, bins=11, label=f\"Class {class_label} ({'ASD' if class_label == 1 else 'No ASD'})\")\n",
    "axes[0].set_xlabel('M-CHAT Total Score', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('M-CHAT Score Distribution by Class', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "autism_data.boxplot(column='MCHAT_Total_Score', by=target_col, ax=axes[1])\n",
    "axes[1].set_xlabel('ASD Class', fontsize=12)\n",
    "axes[1].set_ylabel('M-CHAT Total Score', fontsize=12)\n",
    "axes[1].set_title('M-CHAT Score by ASD Class', fontsize=14, fontweight='bold')\n",
    "plt.sca(axes[1])\n",
    "plt.xticks([1, 2], ['No ASD (0)', 'ASD (1)'])\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=autism_data, x=target_col, y='MCHAT_Total_Score', ax=axes[2])\n",
    "axes[2].set_xlabel('ASD Class', fontsize=12)\n",
    "axes[2].set_ylabel('M-CHAT Total Score', fontsize=12)\n",
    "axes[2].set_title('M-CHAT Score Distribution (Violin)', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xticklabels(['No ASD (0)', 'ASD (1)'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbae21",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Individual M-CHAT Question Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a292a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each M-CHAT question\n",
    "print(\"=\"*80)\n",
    "print(\"M-CHAT QUESTION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "question_stats = []\n",
    "for i in range(1, 11):\n",
    "    col = f'A{i}_Score'\n",
    "    \n",
    "    # Calculate statistics for each class\n",
    "    no_asd_mean = autism_data[autism_data[target_col] == 0][col].mean()\n",
    "    asd_mean = autism_data[autism_data[target_col] == 1][col].mean()\n",
    "    difference = asd_mean - no_asd_mean\n",
    "    \n",
    "    question_stats.append({\n",
    "        'Question': f'A{i}',\n",
    "        'No_ASD_Mean': no_asd_mean,\n",
    "        'ASD_Mean': asd_mean,\n",
    "        'Difference': difference\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  No ASD Mean: {no_asd_mean:.3f}\")\n",
    "    print(f\"  ASD Mean: {asd_mean:.3f}\")\n",
    "    print(f\"  Difference: {difference:.3f}\")\n",
    "\n",
    "question_df = pd.DataFrame(question_stats)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nSummary Table:\")\n",
    "print(question_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize question importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot of mean scores by class\n",
    "x = np.arange(len(question_df))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, question_df['No_ASD_Mean'], width, label='No ASD', alpha=0.8, color='steelblue')\n",
    "axes[0].bar(x + width/2, question_df['ASD_Mean'], width, label='ASD', alpha=0.8, color='coral')\n",
    "axes[0].set_xlabel('M-CHAT Questions', fontsize=12)\n",
    "axes[0].set_ylabel('Mean Score', fontsize=12)\n",
    "axes[0].set_title('Mean Scores by Question and ASD Class', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(question_df['Question'])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Difference plot\n",
    "colors = ['green' if x > 0 else 'red' for x in question_df['Difference']]\n",
    "axes[1].barh(question_df['Question'], question_df['Difference'], color=colors, alpha=0.7)\n",
    "axes[1].set_xlabel('Score Difference (ASD - No ASD)', fontsize=12)\n",
    "axes[1].set_ylabel('M-CHAT Questions', fontsize=12)\n",
    "axes[1].set_title('Score Differences Between Classes', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8b48a8",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bacf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for M-CHAT questions\n",
    "mchat_data = autism_data[mchat_cols + [target_col]]\n",
    "correlation_matrix = mchat_data.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('M-CHAT Questions Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with target\n",
    "print(\"\\nCorrelation with ASD Class:\")\n",
    "target_corr = correlation_matrix[target_col].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed0788",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Prepare Data for Model Training/Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fa03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features: M-CHAT questions (A1-A10) + age\n",
    "feature_cols = [f'A{i}_Score' for i in range(1, 11)] + ['age']\n",
    "X = autism_data[feature_cols].copy()\n",
    "y = autism_data[target_col].copy()\n",
    "\n",
    "# Handle any missing values in age\n",
    "X['age'] = pd.to_numeric(X['age'], errors='coerce')\n",
    "X['age'].fillna(X['age'].median(), inplace=True)\n",
    "\n",
    "print(\"Feature Matrix Shape:\", X.shape)\n",
    "print(\"Target Vector Shape:\", y.shape)\n",
    "print(\"\\nFeatures used:\")\n",
    "print(X.columns.tolist())\n",
    "print(\"\\nFeature Statistics:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c90f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training Set Size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test Set Size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining Set Class Distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nTest Set Class Distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a880160",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Load or Train Autism Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d762f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load existing model\n",
    "try:\n",
    "    with open('models/autism_model.sav', 'rb') as f:\n",
    "        autism_model = pickle.load(f)\n",
    "    print(\"✓ Autism model loaded successfully!\")\n",
    "    model_loaded = True\n",
    "except FileNotFoundError:\n",
    "    print(\"Model not found. Training new model...\")\n",
    "    model_loaded = False\n",
    "\n",
    "if not model_loaded:\n",
    "    # Train Random Forest model with optimized hyperparameters\n",
    "    autism_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=42,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    autism_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model\n",
    "    with open('models/autism_model.sav', 'wb') as f:\n",
    "        pickle.dump(autism_model, f)\n",
    "    print(\"✓ New model trained and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb02a1",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Evaluation - Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d87c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = autism_model.predict(X_train)\n",
    "y_test_pred = autism_model.predict(X_test)\n",
    "y_test_proba = autism_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"=\"*80)\n",
    "print(\"AUTISM MODEL EVALUATION - PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n[TRAINING SET]\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_train, y_train_pred, average='weighted'):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_train, y_train_pred, average='weighted'):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_train, y_train_pred, average='weighted'):.4f}\")\n",
    "\n",
    "print(\"\\n[TEST SET]\")\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_test_pred, average='weighted'):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_test_pred, average='weighted'):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_test_pred, average='weighted'):.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_test, y_test_pred):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED CLASSIFICATION REPORT (Test Set)\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_test, y_test_pred, target_names=['No ASD (0)', 'ASD (1)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739932f9",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7554b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['No ASD (0)', 'ASD (1)'],\n",
    "            yticklabels=['No ASD (0)', 'ASD (1)'])\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Normalized\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['No ASD (0)', 'ASD (1)'],\n",
    "            yticklabels=['No ASD (0)', 'ASD (1)'])\n",
    "axes[1].set_ylabel('Actual', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confusion matrix details\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"True Negatives (TN):  {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP):  {tp}\")\n",
    "print(f\"\\nSensitivity (Recall): {tp/(tp+fn):.4f}\")\n",
    "print(f\"Specificity: {tn/(tn+fp):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534280c2",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. ROC Curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c00813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curve - Autism Risk Assessment Model', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aaafad",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f2c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': autism_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE RANKING\")\n",
    "print(\"=\"*80)\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot\n",
    "axes[0].barh(feature_importance['Feature'], feature_importance['Importance'], color='steelblue')\n",
    "axes[0].set_xlabel('Importance', fontsize=12)\n",
    "axes[0].set_title('Feature Importance - All Features', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Pie chart for top features\n",
    "top_features = feature_importance.head(5)\n",
    "colors_pie = plt.cm.Set3(range(len(top_features)))\n",
    "axes[1].pie(top_features['Importance'], labels=top_features['Feature'], autopct='%1.1f%%',\n",
    "            startangle=90, colors=colors_pie)\n",
    "axes[1].set_title('Top 5 Feature Importance Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc6bda",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-fold cross-validation\n",
    "cv_scores = cross_val_score(autism_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_precision = cross_val_score(autism_model, X_train, y_train, cv=5, scoring='precision_weighted')\n",
    "cv_recall = cross_val_score(autism_model, X_train, y_train, cv=5, scoring='recall_weighted')\n",
    "cv_f1 = cross_val_score(autism_model, X_train, y_train, cv=5, scoring='f1_weighted')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"5-FOLD CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nAccuracy Scores: {cv_scores}\")\n",
    "print(f\"Mean Accuracy:  {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\nPrecision Scores: {cv_precision}\")\n",
    "print(f\"Mean Precision: {cv_precision.mean():.4f} (+/- {cv_precision.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\nRecall Scores: {cv_recall}\")\n",
    "print(f\"Mean Recall:    {cv_recall.mean():.4f} (+/- {cv_recall.std() * 2:.4f})\")\n",
    "\n",
    "print(f\"\\nF1 Scores: {cv_f1}\")\n",
    "print(f\"Mean F1:        {cv_f1.mean():.4f} (+/- {cv_f1.std() * 2:.4f})\")\n",
    "\n",
    "# Visualize CV results\n",
    "cv_results = pd.DataFrame({\n",
    "    'Fold': range(1, 6),\n",
    "    'Accuracy': cv_scores,\n",
    "    'Precision': cv_precision,\n",
    "    'Recall': cv_recall,\n",
    "    'F1 Score': cv_f1\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Line plot\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score']:\n",
    "    axes[0].plot(cv_results['Fold'], cv_results[metric], marker='o', label=metric, linewidth=2)\n",
    "axes[0].set_xlabel('Fold', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Cross-Validation Scores by Fold', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "\n",
    "# Box plot\n",
    "cv_data = pd.DataFrame({\n",
    "    'Accuracy': cv_scores,\n",
    "    'Precision': cv_precision,\n",
    "    'Recall': cv_recall,\n",
    "    'F1 Score': cv_f1\n",
    "})\n",
    "cv_data.boxplot(ax=axes[1])\n",
    "axes[1].set_ylabel('Score', fontsize=12)\n",
    "axes[1].set_title('Cross-Validation Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851b1f1",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a66a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some sample predictions\n",
    "sample_indices = np.random.choice(X_test.index, size=10, replace=False)\n",
    "samples = X_test.loc[sample_indices]\n",
    "samples_pred = autism_model.predict(samples)\n",
    "samples_proba = autism_model.predict_proba(samples)[:, 1]\n",
    "samples_actual = y_test.loc[sample_indices]\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Sample_ID': sample_indices,\n",
    "    'MCHAT_Score': samples[mchat_cols].sum(axis=1),\n",
    "    'Age': samples['age'],\n",
    "    'Actual': samples_actual.map({0: 'No ASD', 1: 'ASD'}),\n",
    "    'Predicted': pd.Series(samples_pred, index=sample_indices).map({0: 'No ASD', 1: 'ASD'}),\n",
    "    'Probability': samples_proba,\n",
    "    'Correct': samples_actual.values == samples_pred\n",
    "})\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS (10 Random Test Cases)\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\n✓ Correctly Predicted: {results_df['Correct'].sum()}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c63e2",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different probability thresholds\n",
    "thresholds_to_test = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    y_pred_threshold = (y_test_proba >= threshold).astype(int)\n",
    "    acc = accuracy_score(y_test, y_pred_threshold)\n",
    "    prec = precision_score(y_test, y_pred_threshold, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_threshold, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_threshold, average='weighted', zero_division=0)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': threshold,\n",
    "        'Accuracy': acc,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1_Score': f1\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"THRESHOLD ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(threshold_df.to_string(index=False))\n",
    "\n",
    "# Visualize threshold impact\n",
    "plt.figure(figsize=(12, 6))\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1_Score']:\n",
    "    plt.plot(threshold_df['Threshold'], threshold_df[metric], marker='o', label=metric, linewidth=2)\n",
    "plt.xlabel('Prediction Threshold', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Model Performance vs Prediction Threshold', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c8c04",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Key Findings and Clinical Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS AND CLINICAL INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"   • Test Accuracy: {accuracy_score(y_test, y_test_pred):.2%}\")\n",
    "print(f\"   • ROC AUC Score: {roc_auc_score(y_test, y_test_proba):.4f}\")\n",
    "print(f\"   • Cross-Validation Mean: {cv_scores.mean():.2%} (+/- {cv_scores.std() * 2:.2%})\")\n",
    "\n",
    "print(\"\\n2. MOST IMPORTANT M-CHAT QUESTIONS:\")\n",
    "for idx, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   • {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\n3. M-CHAT SCORE ANALYSIS:\")\n",
    "mean_score_no_asd = autism_data[autism_data[target_col] == 0]['MCHAT_Total_Score'].mean()\n",
    "mean_score_asd = autism_data[autism_data[target_col] == 1]['MCHAT_Total_Score'].mean()\n",
    "print(f\"   • Average score (No ASD): {mean_score_no_asd:.2f}\")\n",
    "print(f\"   • Average score (ASD): {mean_score_asd:.2f}\")\n",
    "print(f\"   • Difference: {mean_score_asd - mean_score_no_asd:.2f} points\")\n",
    "\n",
    "print(\"\\n4. CLINICAL RECOMMENDATIONS:\")\n",
    "if accuracy_score(y_test, y_test_pred) >= 0.85:\n",
    "    print(\"   ✓ Model shows high accuracy - suitable for screening support\")\n",
    "elif accuracy_score(y_test, y_test_pred) >= 0.75:\n",
    "    print(\"   ✓ Model shows good accuracy - can assist clinical decision-making\")\n",
    "else:\n",
    "    print(\"   ⚠ Model needs improvement - use with caution\")\n",
    "\n",
    "print(\"   • Always combine M-CHAT results with clinical assessment\")\n",
    "print(\"   • Use as a screening tool, not diagnostic tool\")\n",
    "print(\"   • Consider false positives/negatives in clinical context\")\n",
    "\n",
    "print(\"\\n5. MODEL LIMITATIONS:\")\n",
    "print(\"   • M-CHAT is a screening tool, not a diagnostic instrument\")\n",
    "print(\"   • Results depend on parent/caregiver accuracy in responses\")\n",
    "print(\"   • Cultural and linguistic factors may affect responses\")\n",
    "print(\"   • Model should be regularly retrained with new data\")\n",
    "\n",
    "print(\"\\n6. DEPLOYMENT READINESS:\")\n",
    "print(\"   ✓ Model trained and saved successfully\")\n",
    "print(\"   ✓ Feature engineering pipeline established\")\n",
    "print(\"   ✓ Evaluation metrics documented\")\n",
    "print(\"   ✓ Ready for integration with API endpoints\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3185f15",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comprehensive results\n",
    "evaluation_summary = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC', 'MCC', 'CV Mean', 'CV Std'],\n",
    "    'Score': [\n",
    "        accuracy_score(y_test, y_test_pred),\n",
    "        precision_score(y_test, y_test_pred, average='weighted'),\n",
    "        recall_score(y_test, y_test_pred, average='weighted'),\n",
    "        f1_score(y_test, y_test_pred, average='weighted'),\n",
    "        roc_auc_score(y_test, y_test_proba),\n",
    "        matthews_corrcoef(y_test, y_test_pred),\n",
    "        cv_scores.mean(),\n",
    "        cv_scores.std()\n",
    "    ]\n",
    "})\n",
    "\n",
    "evaluation_summary.to_csv('autism_mchat_evaluation_results.csv', index=False)\n",
    "feature_importance.to_csv('autism_mchat_feature_importance.csv', index=False)\n",
    "question_df.to_csv('autism_mchat_question_analysis.csv', index=False)\n",
    "\n",
    "print(\"✓ Evaluation results saved successfully!\")\n",
    "print(\"  • autism_mchat_evaluation_results.csv\")\n",
    "print(\"  • autism_mchat_feature_importance.csv\")\n",
    "print(\"  • autism_mchat_question_analysis.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
